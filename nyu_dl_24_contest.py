# -*- coding: utf-8 -*-
"""NYU_DL_24_Contest_template.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VLjRs9WcVcd7dOxixBOHAyV0Lc0_137-

# Math Question Answer Verification Competition

## Starter Code

Borrowed from [official Unsloth implementation](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing#scrollTo=MKX_XKs_BNZR)
"""

# %%capture
# This cell will take time
!pip install unsloth
# Also get the latest nightly Unsloth!
!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"

from unsloth import FastLanguageModel
import torch
max_seq_length = 768 # Choose any
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Meta-Llama-3.1-8B",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)

"""## Load model and wrap with LoRA adapters"""

model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj", "ffn_proj",
                      "multi_head_attention",
                      "attention_output",
                      "lm_head",
                      "cls_output",
                      "attention_bias",
                     ],
    lora_alpha = 32,
    lora_dropout = 0.1, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

"""## Load dataset"""

# download and load competition dataset

from datasets import load_dataset
dataset = load_dataset("ad6398/nyu-dl-teach-maths-comp")

# print and see dataset
dataset['train'][0]

train_ds = load_dataset("ad6398/nyu-dl-teach-maths-comp", split='train[:4%]')

import pandas as pd
from datasets import Dataset

# balance the dataset
df = pd.DataFrame(train_ds)

total = len(train_ds)
trueCnt=0
for i in range(total):
  if train_ds[i]['is_correct'] == True:
    trueCnt = trueCnt+1
print(trueCnt)

falseCnt = total - trueCnt
if falseCnt > trueCnt:
  indices_to_drop = df[df['is_correct']==False].index[:falseCnt-trueCnt]
  df = df.drop(indices_to_drop).reset_index(drop=True)
else:
  indices_to_drop = df[df['is_correct']==True].index[:trueCnt-falseCnt]
  df = df.drop(indices_to_drop).reset_index(drop=True)

train_ds = Dataset.from_pandas(df)

trueCnt = 0
for i in range(len(train_ds)):
  if train_ds[i]['is_correct'] == True:
    trueCnt = trueCnt+1
print('True sample Number: ', trueCnt, 'False sample Number: ',len(train_ds)-trueCnt)

train_val_split = train_ds.train_test_split(test_size=0.01, seed=42)
train_data = train_val_split["train"]
validation_data = train_val_split["test"]

"""# Analyze DataSet"""

import matplotlib.pyplot as plt
import numpy as np

# Analyze token lengths for the "question" and "solution" fields

question_lengths = [len(tokenizer(question)["input_ids"]) for question in train_ds["question"]]
solution_lengths = [len(tokenizer(solution)["input_ids"]) for solution in train_ds["solution"]]

# Plot token length distributions
plt.hist(question_lengths, bins=50, alpha=0.5, label="Questions")
plt.hist(solution_lengths, bins=50, alpha=0.5, label="Solutions")
plt.xlabel("Token Length")
plt.ylabel("Frequency")
plt.legend()
plt.title("Token Length Distribution")
plt.show()

# Statistics
print(f"Mean question length: {np.mean(question_lengths)}")
print(f"95th percentile question length: {np.percentile(question_lengths, 95)}")
print(f"Mean solution length: {np.mean(solution_lengths)}")
print(f"95th percentile solution length: {np.percentile(solution_lengths, 95)}")

"""# Prepare Training Data"""

prompt = """You are a great mathematician and you are tasked with finding if an answer to a given maths question is correct or not.
Consider whether the solution steps and logic are consistent with mathematical principles.
If the answer and solution are correct, respond with 'True'. If you find any error in the solution, or answer, respond with 'False'.


### Question:
{}

### Answer:
{}

### Solution:
{}

### Output:
{}"""

EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN
def formatting_prompts_func(examples):
    question = examples["question"]
    ans       = examples["answer"]
    solution = examples["solution"]
    output      = examples["is_correct"]
    texts = []
    for instruction, input, sol, output in zip(question, ans, solution, output):
        # Must add EOS_TOKEN, otherwise your generation will go on forever!
        text = prompt.format(instruction, input, sol, output) + EOS_TOKEN
        texts.append(text)
    return { "text" : texts, }

# Process the training dataset and generate prompt for each datapoint
#train_dataset = dataset['train'].map(formatting_prompts_func, batched = True,)
train_dataset = train_data.map(formatting_prompts_func, batched = True,)
validation_dataset = validation_data.map(formatting_prompts_func, batched=True)

#print a smaple training example
train_dataset['text'][0]

"""## SFT"""

from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported

training_args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 100,
        # num_train_epochs = 1, # Set this for 1 full training run.
        max_steps = 1000,
        learning_rate = 2e-5,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 10,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "none", # Use this for WandB etc
        eval_steps=10,                # Evaluate every 10 steps
        evaluation_strategy="steps",  # Use step-based evaluation
        save_strategy="steps",        # Save the model every 10 steps
        save_steps=10,
        load_best_model_at_end=True
    )

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = train_dataset,
    eval_dataset = validation_dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 4,
    packing = False, # Can make training 5x faster for short sequences.
    args = training_args
)

trainer_stats = trainer.train()

model.save_pretrained("lora_model") # Local saving
tokenizer.save_pretrained("lora_model")

"""## Train on Completion Only"""

from trl import DataCollatorForCompletionOnlyLM

response_template = "### Output:"
#collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)
collator = DataCollatorForCompletionOnlyLM(tokenizer.encode(response_template, add_special_tokens = False)[2:], tokenizer=tokenizer)

training_args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 100,
        # num_train_epochs = 1, # Set this for 1 full training run.
        max_steps = 1000,
        learning_rate = 1e-6,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 10,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "none", # Use this for WandB etc
        eval_steps=10,                # Evaluate every 10 steps
        evaluation_strategy="steps",  # Use step-based evaluation
        save_strategy="steps",        # Save the model every 10 steps
        save_steps=10,
        load_best_model_at_end=True
    )


trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = train_dataset,
    eval_dataset = validation_dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 4,
    packing = False, # Can make training 5x faster for short sequences.
    args = training_args,
    data_collator=collator,
)

trainer_stats = trainer.train()

model.save_pretrained("lora_model_completion") # Local saving
tokenizer.save_pretrained("lora_model_completion")

"""## loading model"""

if True:
    from unsloth import FastLanguageModel
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = "lora_model", # YOUR MODEL YOU USED FOR TRAINING
        max_seq_length = max_seq_length,
        dtype = dtype,
        load_in_4bit = load_in_4bit,
    )
    FastLanguageModel.for_inference(model) # Enable native 2x faster inference

"""# Batch Inference

"""

prompt = """You are a great mathematician and you are tasked with finding if an answer to a given maths question is correct or not.
Consider whether the solution steps and logic are consistent with mathematical principles.
If the answer and solution are correct, respond with 'True'. If you find any error in the solution, or answer, respond with 'False'.


### Question:
{}

### Answer:
{}

### Solution:
{}

### Output:
{}"""

def formatting_test_prompts_func(examples):
    question = examples["question"]
    ans       = examples["answer"]
    solution = examples["solution"]
    texts = []
    for instruction, input, sol in zip(question, ans, solution):
        text = prompt.format(instruction, input, sol, "")
        texts.append(text)
    return { "text" : texts, }


test_text_dataset = dataset['test'].map(formatting_test_prompts_func, batched = True,)

# Running inference on batch test
import gc

FastLanguageModel.for_inference(model)

test_dataset = test_text_dataset['text']
batch_size = 8
output_label = []

for i in range(0, len(test_dataset), batch_size):
    #torch.cuda.empty_cache()
    #gc.collect()

    batch = test_dataset[i:i + batch_size]

    # Tokenize batch
    inputs = tokenizer(batch, return_tensors="pt", padding=True, truncation=True).to("cuda")

    with torch.no_grad():
      outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True, num_beams=1, early_stopping=True)

    # Extract only the generated part and decode
    text_generated = tokenizer.batch_decode(outputs, skip_special_tokens=True)

    # Extract the last line (True/False) from each generated text
    for text in text_generated:
        output_label.append(text.splitlines()[-1])

    print(f"batch-num: {min(i + batch_size, len(test_dataset))}/{len(test_dataset)}")

print(len(output_label))

import csv

cnt = 0
csv_data = [["ID", "is_correct"]]
for i in range(len(output_label)):
  if output_label[i] == 'True':
    cnt = cnt + 1
    csv_data.append([i, True])
  else:
    csv_data.append([i, False])

# Write to CSV file
with open('data.csv', 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerows(csv_data)

print("successfully write to data.csv")

from google.colab import files
files.download('data.csv')

!zip -r ./lora_model.zip ./lora_model
!zip -r ./lora_model_completion.zip ./lora_model_completion

files.download('lora_model.zip')
files.download('lora_model_completion.zip')