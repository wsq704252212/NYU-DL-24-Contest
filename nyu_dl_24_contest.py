# -*- coding: utf-8 -*-
"""NYU_DL_24_Contest-testing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18m7byl66MQhSIJcVoSUcGP9vekGvxa1R

# Math Question Answer Verification Competition

## Starter Code

Borrowed from [official Unsloth implementation](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing#scrollTo=MKX_XKs_BNZR)
"""

# %%capture
# This cell will take time
!pip install unsloth
# Also get the latest nightly Unsloth!
!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"

from unsloth import FastLanguageModel
import torch
max_seq_length = 2048 # Choose any
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Meta-Llama-3.1-8B",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)

"""## Load model and wrap with LoRA adapters"""

model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",
                      "multi_head_attention",
                      "attention_output",
                      "lm_head",
                      "cls_output",
                      "attention_bias",
                     ],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

"""## Competition dataset"""

# download and load competition dataset

from datasets import load_dataset
dataset = load_dataset("ad6398/nyu-dl-teach-maths-comp")

train_ds = load_dataset("ad6398/nyu-dl-teach-maths-comp", split='train[:4%]')

import pandas as pd
from datasets import Dataset

df = pd.DataFrame(train_ds)

total = len(train_ds)
trueCnt=0
for i in range(total):
  if train_ds[i]['is_correct'] == True:
    trueCnt = trueCnt+1
print(trueCnt)

falseCnt = total - trueCnt
if falseCnt > trueCnt:
  indices_to_drop = df[df['is_correct']==False].index[:falseCnt-trueCnt]
  df = df.drop(indices_to_drop).reset_index(drop=True)
else:
  indices_to_drop = df[df['is_correct']==True].index[:trueCnt-falseCnt]
  df = df.drop(indices_to_drop).reset_index(drop=True)

train_ds = Dataset.from_pandas(df)

trueCnt = 0
for i in range(len(train_ds)):
  if train_ds[i]['is_correct'] == True:
    trueCnt = trueCnt+1
print(trueCnt, len(train_ds)-trueCnt)

# import pandas as pd
# import numpy as np

# # Convert training data to pandas for easier sampling
# train_df = pd.DataFrame({
#     'question': dataset['train']['question'],
#     'answer': dataset['train']['answer'],
#     'solution': dataset['train']['solution'],
#     'is_correct': dataset['train']['is_correct']
# })

# # Separate True and False samples
# true_samples = train_df[train_df['is_correct'] == True]
# false_samples = train_df[train_df['is_correct'] == False]

# # Sample 20k from each class
# n_per_class = 20000
# balanced_true = true_samples.sample(n=n_per_class, random_state=42)
# balanced_false = false_samples.sample(n=n_per_class, random_state=42)

# # Combine and shuffle
# balanced_df = pd.concat([balanced_true, balanced_false])
# balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)

# # Convert back to HuggingFace dataset format
# from datasets import Dataset
# balanced_dataset = Dataset.from_pandas(balanced_df)

# # Verify the distribution
# print("Class distribution in balanced dataset:")
# print(balanced_dataset.select_columns(['is_correct']).class_encode_column('is_correct').value_counts())

prompt = """You are a great mathematician and you are tasked with finding if an answer to a given maths question is correct or not.
Consider whether the solution steps and logic are consistent with mathematical principles.
If the answer and solution are correct, respond with 'True'. If you find any error in the calculations, or answer, respond with 'False'.


### Question:
{}

### Answer:
{}

### Solution:
{}

### Output:
{}"""

EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN
def formatting_prompts_func(examples):
    question = examples["question"]
    ans       = examples["answer"]
    solution = examples["solution"]
    output      = examples["is_correct"]
    texts = []
    for instruction, input, sol, output in zip(question, ans, solution, output):
        # Must add EOS_TOKEN, otherwise your generation will go on forever!
        text = prompt.format(instruction, input, sol, output) + EOS_TOKEN
        texts.append(text)
    return { "text" : texts, }

# Process the training dataset and generate prompt for each datapoint
train_dataset = train_ds.map(formatting_prompts_func, batched = True,)

# train_dataset_full = dataset['train'].shuffle(seed=42)
# validation_size = int(0.01 * len(train_dataset_full))
# train_dataset = train_dataset_full.select(range(len(train_dataset_full) - validation_size))
# validation_dataset = train_dataset_full.select(range(len(train_dataset_full) - validation_size, len(train_dataset_full)))

# print(f"Main training set size: {len(train_dataset_main)}")
# print(f"Validation set size: {len(validation_dataset)}")

#print a smaple training example
train_dataset['text'][0]

"""## SFT"""

from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported

training_args = TrainingArguments(
        per_device_train_batch_size = 4,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
#         num_train_epochs = 1, # Set this for 1 full training run.
#         warmup_ratio = 0.1,
        max_steps = 25,
        learning_rate = 1e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "none", # Use this for WandB etc
#         gradient_checkpointing=True,
#         max_grad_norm=0.5,
    )

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = train_dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 4,
    packing = False, # Can make training 5x faster for short sequences.
    args = training_args
)

for i in range(10):
  trainer_stats = trainer.train()

model.save_pretrained("lora_model_completion") # Local saving
tokenizer.save_pretrained("lora_model_completion")

"""##Train on Completion Only"""

# from trl import DataCollatorForCompletionOnlyLM

# response_template = "### Output:"
# #collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)
# collator = DataCollatorForCompletionOnlyLM(tokenizer.encode(response_template, add_special_tokens = False)[2:], tokenizer=tokenizer)

# training_args = TrainingArguments(
#         per_device_train_batch_size = 4,
#         gradient_accumulation_steps = 4,
#         warmup_steps = 5,
#         #eval_steps = 20,
#        # num_train_epochs = 1, # Set this for 1 full training run.
#         max_steps = 250,
#         learning_rate = 2e-6,
#         fp16 = not is_bfloat16_supported(),
#         bf16 = is_bfloat16_supported(),
#         logging_steps = 1,
#         optim = "adamw_8bit",
#         weight_decay = 0.01,
#         lr_scheduler_type = "linear",
#         seed = 3407,
#         output_dir = "outputs",
#         report_to = "none", # Use this for WandB etc
#     )


# trainer = SFTTrainer(
#     model = model,
#     tokenizer = tokenizer,
#     train_dataset = train_dataset,
#     #eval_dataset=eval_dataset,
#     dataset_text_field = "text",
#     max_seq_length = max_seq_length,
#     dataset_num_proc = 4,
#     packing = False, # Can make training 5x faster for short sequences.
#     args = training_args,
#     data_collator=collator,
# )

# trainer_stats = trainer.train()

# model.save_pretrained("lora_model_completion") # Local saving
# tokenizer.save_pretrained("lora_model_completion")

"""## inference"""

# Sample inferene data point
dataset = load_dataset("ad6398/nyu-dl-teach-maths-comp")
test_dataset = dataset['test']

sample_ques = test_dataset['question'][0]
sample_ans = test_dataset['answer'][0]
sample_solu = test_dataset['solution'][0]

# Running inference on single test
FastLanguageModel.for_inference(model) # Enable native 2x faster inference
input_prompt = prompt.format(
        sample_ques, # ques
        sample_ans, # given answer
        sample_solu,
        "", # output - leave this blank for generation! LLM willl generate is it is True or False
    )

print("Input Promt:\n", input_prompt)
inputs = tokenizer(
[
    input_prompt
], return_tensors = "pt").to("cuda")

input_shape = inputs['input_ids'].shape
input_token_len = input_shape[1] # 1 because of batch
outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)
# you can get the whole generated text by uncommenting the below line
# text_generated = tokenizer.batch_decode([outputs, skip_special_tokens=True)

response = tokenizer.batch_decode([outputs[0][input_token_len:]], skip_special_tokens=True)
response

"""## loading model"""

if True:
    from unsloth import FastLanguageModel
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = "lora_model_completion", # YOUR MODEL YOU USED FOR TRAINING
        max_seq_length = max_seq_length,
        dtype = dtype,
        load_in_4bit = load_in_4bit,
    )
    FastLanguageModel.for_inference(model) # Enable native 2x faster inference

"""# Batch Inference

"""

prompt = """You are a great mathematician and you are tasked with finding if an answer to a given maths question is correct or not.
Consider whether the solution steps and logic are consistent with mathematical principles.
If the answer and solution are completely correct, respond with 'True'. If you find any error in the calculations, or answer, respond with 'False'.


### Question:
{}

### Answer:
{}

### Solution:
{}

### Output:
{}"""

def formatting_test_prompts_func(examples):
    question = examples["question"]
    ans       = examples["answer"]
    solution = examples["solution"]
    texts = []
    for instruction, input, sol in zip(question, ans, solution):
        text = prompt.format(instruction, input, sol, "")
        texts.append(text)
    return { "text" : texts, }

test_text_dataset = dataset['test'].map(formatting_test_prompts_func, batched = True,)

# Running inference on batch test
import gc

FastLanguageModel.for_inference(model)

test_dataset = test_text_dataset['text']
batch_size = 8
output_label = []

for i in range(0, len(test_dataset), batch_size):
    # torch.cuda.empty_cache()
    # gc.collect()

    batch = test_dataset[i:i + batch_size]

    # Tokenize batch
    inputs = tokenizer(batch, return_tensors="pt", padding=True, truncation=True, max_length=512).to("cuda")

    input_lengths = inputs['input_ids'].shape[1]

    with torch.no_grad():
      outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True, num_beams=1, early_stopping=True)

    # Extract only the generated part and decode
    text_generated = tokenizer.batch_decode(outputs, skip_special_tokens=True)

    # Extract the last line (True/False) from each generated text
    for text in text_generated:
        output_label.append(text.splitlines()[-1])

    print(f"batch-num: {min(i + batch_size, len(test_dataset))}/{len(test_dataset)}")

print(len(output_label))

import csv

cnt = 0
csv_data = [["ID", "is_correct"]]
for i in range(len(output_label)):
  if output_label[i] == 'True':
    cnt = cnt + 1
    csv_data.append([i, True])
  else:
    csv_data.append([i, False])

# Write to CSV file
with open('data.csv', 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerows(csv_data)

print("successfully write to data.csv")

from google.colab import files
files.download('data.csv')

!zip -r ./lora_model.zip ./lora_model
!zip -r ./lora_model_completion.zip ./lora_model_completion

# files.download('lora_model.zip')
files.download('lora_model_completion.zip')