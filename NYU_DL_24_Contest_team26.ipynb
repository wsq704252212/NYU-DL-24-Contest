{"cells":[{"cell_type":"markdown","metadata":{"id":"70hrNJwhYMjR"},"source":["# Math Question Answer Verification Competition\n","\n","## Starter Code"]},{"cell_type":"markdown","metadata":{"id":"kp8dK32_gOZu"},"source":["Borrowed from [official Unsloth implementation](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing#scrollTo=MKX_XKs_BNZR)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"bA1lW9pzWwpk"},"outputs":[],"source":["# %%capture\n","# This cell will take time\n","!pip install unsloth\n","# Also get the latest nightly Unsloth!\n","!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zlpjJOhtW7g3"},"outputs":[],"source":["from unsloth import FastLanguageModel\n","import torch\n","max_seq_length = 768 # Choose any\n","dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n","load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5GxOyBTkXJIG"},"outputs":[],"source":["model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n",")"]},{"cell_type":"markdown","metadata":{"id":"jVgabGjM8G1r"},"source":["## Load model and wrap with LoRA adapters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xy0iN0RJXMAX"},"outputs":[],"source":["model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\", \"ffn_proj\",\n","                      \"multi_head_attention\",\n","                      \"attention_output\",\n","                      \"lm_head\",\n","                      \"cls_output\",\n","                      \"attention_bias\",\n","                     ],\n","    lora_alpha = 32,\n","    lora_dropout = 0.1, # Supports any, but = 0 is optimized\n","    bias = \"none\",    # Supports any, but = \"none\" is optimized\n","    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n","    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n","    random_state = 3407,\n","    use_rslora = False,  # We support rank stabilized LoRA\n","    loftq_config = None, # And LoftQ\n",")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uNruHjDieGSS"},"source":["## Load dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3OMXJz4Z8jhJ"},"outputs":[],"source":["# download and load competition dataset\n","\n","from datasets import load_dataset\n","dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\")\n","\n","# print and see dataset\n","dataset['train'][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JnEBS2DjGWhW"},"outputs":[],"source":["train_ds = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split='train[:4%]')\n","\n","import pandas as pd\n","from datasets import Dataset\n","\n","# balance the dataset\n","df = pd.DataFrame(train_ds)\n","\n","total = len(train_ds)\n","trueCnt=0\n","for i in range(total):\n","  if train_ds[i]['is_correct'] == True:\n","    trueCnt = trueCnt+1\n","print(trueCnt)\n","\n","falseCnt = total - trueCnt\n","if falseCnt > trueCnt:\n","  indices_to_drop = df[df['is_correct']==False].index[:falseCnt-trueCnt]\n","  df = df.drop(indices_to_drop).reset_index(drop=True)\n","else:\n","  indices_to_drop = df[df['is_correct']==True].index[:trueCnt-falseCnt]\n","  df = df.drop(indices_to_drop).reset_index(drop=True)\n","\n","train_ds = Dataset.from_pandas(df)\n","\n","trueCnt = 0\n","for i in range(len(train_ds)):\n","  if train_ds[i]['is_correct'] == True:\n","    trueCnt = trueCnt+1\n","print('True sample Number: ', trueCnt, 'False sample Number: ',len(train_ds)-trueCnt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bvxeIdEj7sLc"},"outputs":[],"source":["train_val_split = train_ds.train_test_split(test_size=0.01, seed=42)\n","train_data = train_val_split[\"train\"]\n","validation_data = train_val_split[\"test\"]"]},{"cell_type":"markdown","metadata":{"id":"TGHdeIgK7sLc"},"source":["# Analyze DataSet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NKWspaj17sLc"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Analyze token lengths for the \"question\" and \"solution\" fields\n","\n","question_lengths = [len(tokenizer(question)[\"input_ids\"]) for question in train_ds[\"question\"]]\n","solution_lengths = [len(tokenizer(solution)[\"input_ids\"]) for solution in train_ds[\"solution\"]]\n","\n","# Plot token length distributions\n","plt.hist(question_lengths, bins=50, alpha=0.5, label=\"Questions\")\n","plt.hist(solution_lengths, bins=50, alpha=0.5, label=\"Solutions\")\n","plt.xlabel(\"Token Length\")\n","plt.ylabel(\"Frequency\")\n","plt.legend()\n","plt.title(\"Token Length Distribution\")\n","plt.show()\n","\n","# Statistics\n","print(f\"Mean question length: {np.mean(question_lengths)}\")\n","print(f\"95th percentile question length: {np.percentile(question_lengths, 95)}\")\n","print(f\"Mean solution length: {np.mean(solution_lengths)}\")\n","print(f\"95th percentile solution length: {np.percentile(solution_lengths, 95)}\")"]},{"cell_type":"markdown","metadata":{"id":"lXu4ZtD57sLd"},"source":["# Prepare Training Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DBpDwJA-bJ9K"},"outputs":[],"source":["prompt = \"\"\"You are a great mathematician and you are tasked with finding if an answer to a given maths question is correct or not.\n","Based on the question, solution and answer, consider whether the solution steps and logic are consistent with mathematical principles.\n","If the answer and solution are correct, respond with 'True'. If you find any error in the solution, or answer, respond with 'False'.\n","\n","\n","### Question:\n","{}\n","\n","### Answer:\n","{}\n","\n","### Solution:\n","{}\n","\n","### Output:\n","{}\"\"\"\n","\n","EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n","def formatting_prompts_func(examples):\n","    question = examples[\"question\"]\n","    ans       = examples[\"answer\"]\n","    solution = examples[\"solution\"]\n","    output      = examples[\"is_correct\"]\n","    texts = []\n","    for instruction, input, sol, output in zip(question, ans, solution, output):\n","        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n","        text = prompt.format(instruction, input, sol, output) + EOS_TOKEN\n","        texts.append(text)\n","    return { \"text\" : texts, }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fEeHyA68-puB"},"outputs":[],"source":["# Process the training dataset and generate prompt for each datapoint\n","train_dataset = train_data.map(formatting_prompts_func, batched = True,)\n","validation_dataset = validation_data.map(formatting_prompts_func, batched=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JKBG7s8woNuo"},"outputs":[],"source":["# print a smaple training example\n","train_dataset['text'][0]"]},{"cell_type":"markdown","metadata":{"id":"egSQOrCJeM7n"},"source":["## SFT"]},{"cell_type":"code","source":["from transformers import EarlyStoppingCallback\n","\n","early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=200)"],"metadata":{"id":"h2Vo_4CY8Ls4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"INoSdVrEbO9Q"},"outputs":[],"source":["from trl import SFTTrainer\n","from transformers import TrainingArguments\n","from unsloth import is_bfloat16_supported\n","\n","training_args = TrainingArguments(\n","        per_device_train_batch_size = 4,\n","        gradient_accumulation_steps = 4,\n","        # warmup_steps = 100,\n","        warmup_ratio = 0.03,\n","        num_train_epochs = 3, # Set this for 1 full training run.\n","        # max_steps = 2000,\n","        learning_rate = 4e-5,\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        logging_steps = 100,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.05,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","        report_to = \"none\", # Use this for WandB etc\n","        eval_steps=200,                # Evaluate every 10 steps\n","        evaluation_strategy=\"steps\",  # Use step-based evaluation\n","        save_strategy=\"steps\",        # Save the model every 10 steps\n","        save_steps=1000,\n","        save_total_limit=2,\n","        load_best_model_at_end=True\n","    )\n","\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = train_dataset,\n","    eval_dataset = validation_dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 4,\n","    packing = False, # Can make training 5x faster for short sequences.\n","    args = training_args,\n","    callbacks=[early_stopping_callback]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WquBPTm4b-3z","collapsed":true},"outputs":[],"source":["trainer_stats = trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mdsC3U8U4mVI"},"outputs":[],"source":["model.save_pretrained(\"lora_model\") # Local saving\n","tokenizer.save_pretrained(\"lora_model\")"]},{"cell_type":"markdown","source":["## For testing: Further fine-tuning based on previous model"],"metadata":{"id":"GGvAufS_f5NL"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM\n","\n","# Load the model from the latest checkpoint\n","model = AutoModelForCausalLM.from_pretrained(\"outputs/checkpoint-5907\")"],"metadata":{"id":"oI8Naku3f4Xh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A7ZqhA6Q4mVI"},"source":["## Train on Completion Only"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zw3YkcMh4mVI"},"outputs":[],"source":["from trl import DataCollatorForCompletionOnlyLM\n","\n","response_template = \"### Output:\"\n","#collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n","collator = DataCollatorForCompletionOnlyLM(tokenizer.encode(response_template, add_special_tokens = False)[2:], tokenizer=tokenizer)\n","\n","training_args = TrainingArguments(\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 4,\n","        warmup_steps = 20,\n","        # num_train_epochs = 1, # Set this for 1 full training run.\n","        max_steps = 1000,\n","        learning_rate = 1e-6,\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        logging_steps = 10,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","        report_to = \"none\", # Use this for WandB etc\n","        eval_steps=20,                # Evaluate every 10 steps\n","        evaluation_strategy=\"steps\",  # Use step-based evaluation\n","        save_strategy=\"steps\",        # Save the model every 10 steps\n","        save_steps=20,\n","        load_best_model_at_end=True\n","    )\n","\n","\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = train_dataset,\n","    eval_dataset = validation_dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 4,\n","    packing = False, # Can make training 5x faster for short sequences.\n","    args = training_args,\n","    data_collator=collator,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OvUVAYi94mVI"},"outputs":[],"source":["trainer_stats = trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hwKkBfKd4mVI"},"outputs":[],"source":["model.save_pretrained(\"lora_model_completion\") # Local saving\n","tokenizer.save_pretrained(\"lora_model_completion\")"]},{"cell_type":"markdown","metadata":{"id":"pKt3vZoSeRvb"},"source":["## loading model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"paHfJLfVccmN"},"outputs":[],"source":["if True:\n","    from unsloth import FastLanguageModel\n","    model, tokenizer = FastLanguageModel.from_pretrained(\n","        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n","        max_seq_length = max_seq_length,\n","        dtype = dtype,\n","        load_in_4bit = load_in_4bit,\n","    )\n","    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n"]},{"cell_type":"markdown","metadata":{"id":"liPyZM3pbYgz"},"source":["# Batch Inference\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tE5mRGldOt_w"},"outputs":[],"source":["prompt = \"\"\"You are a great mathematician and you are tasked with finding if an answer to a given maths question is correct or not.\n","Based on the question, solution and answer, consider whether the solution steps and logic are consistent with mathematical principles.\n","If the answer and solution are correct, respond with 'True'. If you find any error in the solution, or answer, respond with 'False'.\n","\n","\n","### Question:\n","{}\n","\n","### Answer:\n","{}\n","\n","### Solution:\n","{}\n","\n","### Output:\n","{}\"\"\"\n","\n","def formatting_test_prompts_func(examples):\n","    question = examples[\"question\"]\n","    ans       = examples[\"answer\"]\n","    solution = examples[\"solution\"]\n","    texts = []\n","    for instruction, input, sol in zip(question, ans, solution):\n","        text = prompt.format(instruction, input, sol, \"\")\n","        texts.append(text)\n","    return { \"text\" : texts, }\n","\n","\n","test_text_dataset = dataset['test'].map(formatting_test_prompts_func, batched = True,)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jMEvTYU92HNJ"},"outputs":[],"source":["# Running inference on batch test\n","import gc\n","\n","FastLanguageModel.for_inference(model)\n","\n","test_dataset = test_text_dataset['text']\n","batch_size = 8\n","output_label = []\n","\n","for i in range(0, len(test_dataset), batch_size):\n","    #torch.cuda.empty_cache()\n","    #gc.collect()\n","\n","    batch = test_dataset[i:i + batch_size]\n","\n","    # Tokenize batch\n","    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n","\n","    with torch.no_grad():\n","      outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True, num_beams=1, early_stopping=True)\n","\n","    # Extract only the generated part and decode\n","    text_generated = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","\n","    # Extract the last line (True/False) from each generated text\n","    for text in text_generated:\n","        output_label.append(text.splitlines()[-1])\n","\n","    print(f\"batch-num: {min(i + batch_size, len(test_dataset))}/{len(test_dataset)}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A0hjkvVTA-gS"},"outputs":[],"source":["print(len(output_label))\n","\n","import csv\n","\n","cnt = 0\n","csv_data = [[\"ID\", \"is_correct\"]]\n","for i in range(len(output_label)):\n","  if output_label[i] == 'True':\n","    cnt = cnt + 1\n","    csv_data.append([i, True])\n","  else:\n","    csv_data.append([i, False])\n","\n","# Write to CSV file\n","with open('data.csv', 'w', newline='') as file:\n","    writer = csv.writer(file)\n","    writer.writerows(csv_data)\n","\n","print(\"successfully write to data.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-qOtlJQY6fB6"},"outputs":[],"source":["from google.colab import files\n","files.download('data.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Va5dtGA-4mVJ"},"outputs":[],"source":["!zip -r ./lora_model.zip ./lora_model\n","!zip -r ./lora_model_completion.zip ./lora_model_completion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OETPEuPw4mVJ"},"outputs":[],"source":["files.download('lora_model.zip')\n","files.download('lora_model_completion.zip')"]},{"cell_type":"markdown","source":["## Download checkpoint"],"metadata":{"id":"as9kY-JJgx5E"}},{"cell_type":"code","source":["!tar -czvf checkpoint-5800.tar.gz outputs/checkpoint-5907"],"metadata":{"id":"mbADPOq3gwwr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","files.download(\"checkpoint-5800.tar.gz\")"],"metadata":{"id":"G_wfk7kaiWWH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Copy the checkpoint to Google Drive\n","!cp checkpoint-5800.tar.gz /content/drive/MyDrive/"],"metadata":{"id":"DnpzInEJiasb"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}